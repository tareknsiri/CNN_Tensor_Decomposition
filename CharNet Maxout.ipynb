{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import scipy.io as sc\n",
    "import torch\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from time import process_time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#from utils import *\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining ICDAR unpacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICDAR_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file, lim=-1, shuffle=True, transform=None):\n",
    "        self.transform = transform\n",
    "        frame = sc.loadmat(file)\n",
    "        alpha = alphabet()\n",
    "        s, t = [], []\n",
    "\n",
    "        if lim != -1:\n",
    "            n_samples, res = lim // 36, lim % 36\n",
    "        else:\n",
    "            n_samples, res = 100000, 0\n",
    "\n",
    "        for i in range(len(frame[\"images\"][0])):  # All the categories\n",
    "            char_list = frame[\"images\"][0][i][0]\n",
    "            for j in range(min(n_samples, len(char_list))):\n",
    "                s.append(char_list[j])\n",
    "                t.append(alpha[frame[\"labels\"][0][i][0]])\n",
    "\n",
    "        self.samples = torch.tensor(s).float()\n",
    "        self.targets = torch.tensor(t)\n",
    "\n",
    "        # uniqueValues, occurCount = np.unique(self.targets, return_counts=True)\n",
    "        # print(uniqueValues, occurCount)\n",
    "\n",
    "        self.len = len(self.samples)\n",
    "        self.indexes = [i for i in range(len(self.samples))]\n",
    "\n",
    "        if shuffle:\n",
    "            rd.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.samples[self.indexes[idx]]\n",
    "        label = self.targets[self.indexes[idx]]\n",
    "\n",
    "        img.type(torch.uint8)\n",
    "        target = int(label)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = Image.fromarray(img.numpy())\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def shuffle(self):\n",
    "        return rd.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "def alphabet():\n",
    "    alpha = {}\n",
    "    for i in range(10):\n",
    "        alpha[str(i)] = i\n",
    "\n",
    "    for i in range(26):\n",
    "        alpha[chr(65 + i)] = i + 10\n",
    "\n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the decomposing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor_decomposer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(tensor_decomposer, self).__init__()\n",
    "        self.approx = 0  # Approximation error (mean approximation if multiple decompositions)\n",
    "        self.approx_list = []  # List of approximations in case of several decompositions\n",
    "        self.dim_drop = 0  # Parameters reduction\n",
    "        self.original_param_list = []  # List of original parameters (in case of multiple layers)\n",
    "        self.new_param_list = []  # List of new parameters (in case of multiple layers)\n",
    "\n",
    "    def cp_decomposition_conv_layer(self, layer, rank):\n",
    "\n",
    "        t = tl.tensor(layer.weight.data)\n",
    "        a, b, c, d = np.shape(t)\n",
    "        dim_t = a * b * c * d\n",
    "\n",
    "        # Perform CP decomposition on the layer weight tensorly.\n",
    "        dec = parafac(t, rank=rank, init='random')\n",
    "\n",
    "        recomp_tensor = tl.kruskal_to_tensor(dec)\n",
    "\n",
    "        norm_t = np.linalg.norm(t)\n",
    "        self.approx_list.append(np.linalg.norm(recomp_tensor - t) / norm_t)\n",
    "        self.approx = np.mean(self.approx_list)\n",
    "\n",
    "        self.original_param_list.append(dim_t)\n",
    "        self.new_param_list.append(rank * (a + b + c + d))\n",
    "        self.dim_drop = sum(self.original_param_list) / sum(self.new_param_list)\n",
    "\n",
    "        for i in range(len(dec.factors)):\n",
    "            dec.factors[i] = torch.tensor(dec.factors[i])\n",
    "\n",
    "        last, first, vertical, horizontal = dec.factors\n",
    "\n",
    "        pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels=first.shape[0],\n",
    "                                                 out_channels=first.shape[1], kernel_size=1, stride=1, padding=0,\n",
    "                                                 dilation=layer.dilation, bias=False)\n",
    "\n",
    "        depthwise_vertical_layer = torch.nn.Conv2d(in_channels=vertical.shape[1],\n",
    "                                                   out_channels=vertical.shape[1], kernel_size=(vertical.shape[0], 1),\n",
    "                                                   stride=1, padding=(layer.padding[0], 0), dilation=layer.dilation,\n",
    "                                                   groups=vertical.shape[1], bias=False)\n",
    "\n",
    "        depthwise_horizontal_layer = \\\n",
    "            torch.nn.Conv2d(in_channels=horizontal.shape[1],\n",
    "                            out_channels=horizontal.shape[1],\n",
    "                            kernel_size=(1, horizontal.shape[0]), stride=layer.stride,\n",
    "                            padding=(0, layer.padding[0]),\n",
    "                            dilation=layer.dilation, groups=horizontal.shape[1], bias=False)\n",
    "\n",
    "        pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels=last.shape[1],\n",
    "                                                 out_channels=last.shape[0], kernel_size=1, stride=1,\n",
    "                                                 padding=0, dilation=layer.dilation, bias=True)\n",
    "\n",
    "        pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
    "\n",
    "        depthwise_horizontal_layer.weight.data = \\\n",
    "            torch.transpose(horizontal, 1, 0).unsqueeze(1).unsqueeze(1)\n",
    "        depthwise_vertical_layer.weight.data = \\\n",
    "            torch.transpose(vertical, 1, 0).unsqueeze(1).unsqueeze(-1)\n",
    "        pointwise_s_to_r_layer.weight.data = \\\n",
    "            torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
    "        pointwise_r_to_t_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer,\n",
    "                      depthwise_horizontal_layer, pointwise_r_to_t_layer]\n",
    "\n",
    "        return nn.Sequential(*new_layers)\n",
    "\n",
    "    @staticmethod\n",
    "    def recomposition_conv_layer(recomp_tensor):\n",
    "\n",
    "        out_channels, in_channels, kernel_w, kernel_h = np.shape(recomp_tensor)\n",
    "        kernel_size = (kernel_w, kernel_h)\n",
    "        new_layer = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "        new_layer.weight = torch.nn.Parameter(torch.tensor(recomp_tensor))\n",
    "        return new_layer\n",
    "\n",
    "    def plot_approx_err(self, t, r):\n",
    "        norm_t = np.linalg.norm(t)\n",
    "        if isinstance(r, list):\n",
    "            errs = []\n",
    "            for rank in r:\n",
    "                print('\\n ------ rank = {} ------\\n'.format(rank))\n",
    "                f = self.decompose(t, rank)\n",
    "                new_t = self.recompose(f)\n",
    "                err = np.linalg.norm(t - new_t) / norm_t\n",
    "                print(err)\n",
    "                errs.append(err)\n",
    "\n",
    "            plt.plot(r, errs)\n",
    "            plt.xlabel(\"rank values\")\n",
    "            plt.ylabel(\"Approximation error\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            f = self.decompose(t)\n",
    "            new_t = self.recompose(f)\n",
    "            errs = np.linalg.norm(t - new_t) / norm_t\n",
    "        return errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining CharNet classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "\n",
    "\n",
    "class CharNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharNet, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def exec(self, x_train, x_test, optimizer, n_epochs, device = \"cpu\"):\n",
    "        start_time = process_time()\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "        test_losses = []\n",
    "        accuracy_score = []\n",
    "        test_counter = [i * len(x_train.dataset) for i in range(n_epochs + 1)]\n",
    "        self._test(x_test, test_losses, accuracy_score, device)  # To check the random choice option\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            self._train(epoch, x_train, optimizer, train_losses, train_counter, device)\n",
    "\n",
    "            self._test(x_test, test_losses, accuracy_score, device)\n",
    "\n",
    "        print(\"\\n Total Time Elapsed ({} epochs) : {:.3f}\".format(n_epochs, process_time() - start_time))\n",
    "        return None\n",
    "\n",
    "    def plot_res(self, train_losses, train_counter, test_losses, test_counter, accuracy_score):\n",
    "\n",
    "        # Likelihood loss (part III)\n",
    "        plt.plot(train_counter, train_losses, color='blue')\n",
    "        plt.scatter(test_counter, test_losses, color='red')\n",
    "        plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "        plt.xlabel('number of training examples seen')\n",
    "        plt.ylabel('negative log likelihood loss')\n",
    "\n",
    "        # Accuracy (part III)\n",
    "        plt.figure()\n",
    "        plt.plot(accuracy_score)\n",
    "        plt.xlabel('Number of epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        return plt.show()\n",
    "\n",
    "    def _train(self, epoch, x_train, optimizer, train_losses, train_counter, device=\"cpu\"):\n",
    "        self.train()\n",
    "        for batch_idx, (data, target) in enumerate(x_train):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = self.forward(data)\n",
    "            loss = self.loss_fn(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(x_train.dataset),\n",
    "                           100. * batch_idx / len(x_train), loss.item()))\n",
    "                train_losses.append(loss.item())#self._test(x_train, [], [])\n",
    "                train_counter.append(\n",
    "                    (batch_idx * 64) + ((epoch - 1) * len(x_train.dataset)))\n",
    "\n",
    "    def _test(self, x_test, test_losses, accuracy_score, device=\"cpu\"):\n",
    "        start = process_time()\n",
    "        self.eval()  # network in evaluation mode (for batchnorm and dropout layers)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():  # deactivate the autograd engine to reduce memory usage and speed up\n",
    "            for (data, target) in x_test:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = self.forward(data)  # prediction with the CharNet\n",
    "                test_loss = self.loss_fn(output, target)  # Add the negative log likelihood loss.\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(x_test.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        score = int(correct) / len(x_test.dataset)\n",
    "        accuracy_score.append(score)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss.item(), correct, len(x_test.dataset),\n",
    "            100. * score))\n",
    "        return test_loss, score, process_time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListModule(object):\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "\n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i < 0 or i >= self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharNet with Maxout activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNetMaxout(CharNet):\n",
    "    def __init__(self, dataset):\n",
    "        super(CharNetMaxout, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.conv1_list = ListModule(self, \"conv1_\")\n",
    "        self.conv2_list = ListModule(self, \"conv2_\")\n",
    "        self.conv3_list = ListModule(self, \"conv3_\")\n",
    "        self.conv4_list = ListModule(self, \"conv4_\")\n",
    "        for i in range(2):\n",
    "            if dataset == \"CIFAR\":\n",
    "                self.conv1_list.append(nn.Conv2d(3, 48, kernel_size=9))\n",
    "            else:\n",
    "                self.conv1_list.append(nn.Conv2d(1, 48, kernel_size=9))\n",
    "            self.conv2_list.append(nn.Conv2d(48, 64, kernel_size=9))\n",
    "        for i in range(4):\n",
    "            self.conv3_list.append(nn.Conv2d(64, 128, kernel_size=8))\n",
    "            self.conv4_list.append(nn.Conv2d(128, 36, kernel_size=1))\n",
    "        #deffing input by charset\n",
    "        if dataset == \"CIFAR\":\n",
    "            self.fc1 = nn.Linear(36 * 9 * 9, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "        elif dataset == \"ICDAR\":\n",
    "            self.fc1 = nn.Linear(36, 50)\n",
    "            self.fc2 = nn.Linear(50, 36)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(36 * 5 * 5, 50)\n",
    "            self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.maxout(x, self.conv1_list)\n",
    "        x = self.maxout(x, self.conv2_list)\n",
    "        x = self.maxout(x, self.conv3_list)\n",
    "        x = self.maxout(x, self.conv4_list)\n",
    "        if self.dataset == \"CIFAR\":\n",
    "            x = x.view(-1, 36 * 9 * 9)\n",
    "        elif self.dataset == \"ICDAR\":\n",
    "            x = x.view(-1, 36)\n",
    "        else:\n",
    "            x = x.view(-1, 36 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, training=self.training)  # add some random zeros following a Bernoulli distribution\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def maxout(self, x, layer_list):  # Return the max layer among a layer list\n",
    "        max_output = layer_list[0](x)\n",
    "        for _, layer in enumerate(layer_list, start=1):\n",
    "            max_output = torch.max(max_output, layer(x))\n",
    "        return max_output\n",
    "\n",
    "    def save(self, optimizer, model_name):\n",
    "        # Save model and optimizer\n",
    "        torch.save(self.state_dict(), './models/charnet/' + model_name + '.pth')\n",
    "        torch.save(optimizer.state_dict(),\n",
    "                   './models/charnet/' + model_name + '_optimizer.pth')\n",
    "\n",
    "    def decompose(self, n_layer, rank):\n",
    "        decomp = tensor_decomposer()\n",
    "        if n_layer == 0:\n",
    "            new_conv2_list = ListModule(self, \"conv2_\")\n",
    "            new_conv3_list = ListModule(self, \"conv3_\")\n",
    "            for i in range(len(self.conv2_list)):\n",
    "                new_conv2_list.append(decomp.cp_decomposition_conv_layer(self.conv2_list[i], rank))\n",
    "            for j in range(len(self.conv3_list)):\n",
    "                new_conv3_list.append(decomp.cp_decomposition_conv_layer(self.conv3_list[j], rank))\n",
    "            self.conv2_list = new_conv2_list\n",
    "            self.conv3_list = new_conv3_list\n",
    "        elif n_layer == 2:\n",
    "            new_conv2_list = ListModule(self, \"conv2_\")\n",
    "            for i in range(len(self.conv2_list)):\n",
    "                new_conv2_list.append(decomp.cp_decomposition_conv_layer(self.conv2_list[i], rank))\n",
    "            self.conv2_list = new_conv2_list\n",
    "        elif n_layer == 3:\n",
    "            new_conv3_list = ListModule(self, \"conv3_\")\n",
    "            for j in range(len(self.conv3_list)):\n",
    "                new_conv3_list.append(decomp.cp_decomposition_conv_layer(self.conv3_list[j], rank))\n",
    "            self.conv3_list = new_conv3_list\n",
    "        else:\n",
    "            print(\"Invalid layer index (use 0, 2 or 3)\")\n",
    "\n",
    "        return decomp.approx, decomp.dim_drop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and pre-precess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"./datasets/ICDAR/case-insensitive-train-v7.mat\"\n",
    "path_test = \"./datasets/ICDAR/icdar2003-chars-test-v7.mat\"\n",
    "\n",
    "unpacker = ICDAR_Dataset('./datasets/ICDAR/icdar2003-chars-test-v7.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_icdar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,))])\n",
    "train_data = ICDAR_Dataset(path_train, shuffle=True, transform=transform_icdar)\n",
    "test_data = ICDAR_Dataset(path_test, shuffle=True, transform=transform_icdar)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=256, num_workers=0,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=256, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning a Maxout CharNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Launching the training process...\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 63/5198 (1%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 24.835514\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 3.502336\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 3.102831\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 2.599350\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 2.558714\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 2.360956\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 2.268307\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 2.142361\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.993094\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.855939\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.782618\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.624043\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.880560\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.755664\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.632275\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3545/5198 (68%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.676288\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.405105\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.575456\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.510680\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.333005\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.434449\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.200665\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.221200\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.220792\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.076155\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.273960\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.290059\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.131929\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.050264\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.103722\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "Train Epoch: 3 [0/185639 (0%)]\tLoss: 0.979323\n",
      "Train Epoch: 3 [12800/185639 (7%)]\tLoss: 0.970019\n",
      "Train Epoch: 3 [25600/185639 (14%)]\tLoss: 1.148823\n",
      "Train Epoch: 3 [38400/185639 (21%)]\tLoss: 1.190086\n",
      "Train Epoch: 3 [51200/185639 (28%)]\tLoss: 0.980678\n",
      "Train Epoch: 3 [64000/185639 (34%)]\tLoss: 0.978880\n",
      "Train Epoch: 3 [76800/185639 (41%)]\tLoss: 0.976177\n",
      "Train Epoch: 3 [89600/185639 (48%)]\tLoss: 0.838898\n",
      "Train Epoch: 3 [102400/185639 (55%)]\tLoss: 1.010591\n",
      "Train Epoch: 3 [115200/185639 (62%)]\tLoss: 0.873536\n",
      "Train Epoch: 3 [128000/185639 (69%)]\tLoss: 0.822400\n",
      "Train Epoch: 3 [140800/185639 (76%)]\tLoss: 0.947062\n",
      "Train Epoch: 3 [153600/185639 (83%)]\tLoss: 1.018399\n",
      "Train Epoch: 3 [166400/185639 (90%)]\tLoss: 0.812939\n",
      "Train Epoch: 3 [179200/185639 (96%)]\tLoss: 0.893214\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4110/5198 (79%)\n",
      "\n",
      "Train Epoch: 4 [0/185639 (0%)]\tLoss: 0.932563\n",
      "Train Epoch: 4 [12800/185639 (7%)]\tLoss: 1.035720\n",
      "Train Epoch: 4 [25600/185639 (14%)]\tLoss: 0.790052\n",
      "Train Epoch: 4 [38400/185639 (21%)]\tLoss: 0.855119\n",
      "Train Epoch: 4 [51200/185639 (28%)]\tLoss: 0.778190\n",
      "Train Epoch: 4 [64000/185639 (34%)]\tLoss: 0.792283\n",
      "Train Epoch: 4 [76800/185639 (41%)]\tLoss: 1.088584\n",
      "Train Epoch: 4 [89600/185639 (48%)]\tLoss: 0.909899\n",
      "Train Epoch: 4 [102400/185639 (55%)]\tLoss: 0.744164\n",
      "Train Epoch: 4 [115200/185639 (62%)]\tLoss: 0.886782\n",
      "Train Epoch: 4 [128000/185639 (69%)]\tLoss: 0.764530\n",
      "Train Epoch: 4 [140800/185639 (76%)]\tLoss: 0.669604\n",
      "Train Epoch: 4 [153600/185639 (83%)]\tLoss: 0.814808\n",
      "Train Epoch: 4 [166400/185639 (90%)]\tLoss: 0.778872\n",
      "Train Epoch: 4 [179200/185639 (96%)]\tLoss: 0.849094\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4152/5198 (80%)\n",
      "\n",
      "Train Epoch: 5 [0/185639 (0%)]\tLoss: 0.710687\n",
      "Train Epoch: 5 [12800/185639 (7%)]\tLoss: 0.756254\n",
      "Train Epoch: 5 [25600/185639 (14%)]\tLoss: 0.703076\n",
      "Train Epoch: 5 [38400/185639 (21%)]\tLoss: 0.601173\n",
      "Train Epoch: 5 [51200/185639 (28%)]\tLoss: 0.686195\n",
      "Train Epoch: 5 [64000/185639 (34%)]\tLoss: 0.828905\n",
      "Train Epoch: 5 [76800/185639 (41%)]\tLoss: 0.808316\n",
      "Train Epoch: 5 [89600/185639 (48%)]\tLoss: 0.670814\n",
      "Train Epoch: 5 [102400/185639 (55%)]\tLoss: 0.532193\n",
      "Train Epoch: 5 [115200/185639 (62%)]\tLoss: 0.834892\n",
      "Train Epoch: 5 [128000/185639 (69%)]\tLoss: 0.858300\n",
      "Train Epoch: 5 [140800/185639 (76%)]\tLoss: 0.858794\n",
      "Train Epoch: 5 [153600/185639 (83%)]\tLoss: 0.634877\n",
      "Train Epoch: 5 [166400/185639 (90%)]\tLoss: 0.559780\n",
      "Train Epoch: 5 [179200/185639 (96%)]\tLoss: 0.836416\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4248/5198 (82%)\n",
      "\n",
      "Train Epoch: 6 [0/185639 (0%)]\tLoss: 0.532338\n",
      "Train Epoch: 6 [12800/185639 (7%)]\tLoss: 0.575081\n",
      "Train Epoch: 6 [25600/185639 (14%)]\tLoss: 0.558122\n",
      "Train Epoch: 6 [38400/185639 (21%)]\tLoss: 0.584830\n",
      "Train Epoch: 6 [51200/185639 (28%)]\tLoss: 0.703039\n",
      "Train Epoch: 6 [64000/185639 (34%)]\tLoss: 0.671812\n",
      "Train Epoch: 6 [76800/185639 (41%)]\tLoss: 0.583276\n",
      "Train Epoch: 6 [89600/185639 (48%)]\tLoss: 0.721344\n",
      "Train Epoch: 6 [102400/185639 (55%)]\tLoss: 0.586247\n",
      "Train Epoch: 6 [115200/185639 (62%)]\tLoss: 0.650618\n",
      "Train Epoch: 6 [128000/185639 (69%)]\tLoss: 0.798144\n",
      "Train Epoch: 6 [140800/185639 (76%)]\tLoss: 0.574128\n",
      "Train Epoch: 6 [153600/185639 (83%)]\tLoss: 0.758435\n",
      "Train Epoch: 6 [166400/185639 (90%)]\tLoss: 0.625911\n",
      "Train Epoch: 6 [179200/185639 (96%)]\tLoss: 0.657438\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4305/5198 (83%)\n",
      "\n",
      "Train Epoch: 7 [0/185639 (0%)]\tLoss: 0.414020\n",
      "Train Epoch: 7 [12800/185639 (7%)]\tLoss: 0.479711\n",
      "Train Epoch: 7 [25600/185639 (14%)]\tLoss: 0.635947\n",
      "Train Epoch: 7 [38400/185639 (21%)]\tLoss: 0.521767\n",
      "Train Epoch: 7 [51200/185639 (28%)]\tLoss: 0.677046\n",
      "Train Epoch: 7 [64000/185639 (34%)]\tLoss: 0.516886\n",
      "Train Epoch: 7 [76800/185639 (41%)]\tLoss: 0.581439\n",
      "Train Epoch: 7 [89600/185639 (48%)]\tLoss: 0.560759\n",
      "Train Epoch: 7 [102400/185639 (55%)]\tLoss: 0.475223\n",
      "Train Epoch: 7 [115200/185639 (62%)]\tLoss: 0.481804\n",
      "Train Epoch: 7 [128000/185639 (69%)]\tLoss: 0.534564\n",
      "Train Epoch: 7 [140800/185639 (76%)]\tLoss: 0.734717\n",
      "Train Epoch: 7 [153600/185639 (83%)]\tLoss: 0.542432\n",
      "Train Epoch: 7 [166400/185639 (90%)]\tLoss: 0.751615\n",
      "Train Epoch: 7 [179200/185639 (96%)]\tLoss: 0.535986\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4283/5198 (82%)\n",
      "\n",
      "Train Epoch: 8 [0/185639 (0%)]\tLoss: 0.417682\n",
      "Train Epoch: 8 [12800/185639 (7%)]\tLoss: 0.551013\n",
      "Train Epoch: 8 [25600/185639 (14%)]\tLoss: 0.438485\n",
      "Train Epoch: 8 [38400/185639 (21%)]\tLoss: 0.627428\n",
      "Train Epoch: 8 [51200/185639 (28%)]\tLoss: 0.444948\n",
      "Train Epoch: 8 [64000/185639 (34%)]\tLoss: 0.556739\n",
      "Train Epoch: 8 [76800/185639 (41%)]\tLoss: 0.481761\n",
      "Train Epoch: 8 [89600/185639 (48%)]\tLoss: 0.596189\n",
      "Train Epoch: 8 [102400/185639 (55%)]\tLoss: 0.620018\n",
      "Train Epoch: 8 [115200/185639 (62%)]\tLoss: 0.586160\n",
      "Train Epoch: 8 [128000/185639 (69%)]\tLoss: 0.537925\n",
      "Train Epoch: 8 [140800/185639 (76%)]\tLoss: 0.458367\n",
      "Train Epoch: 8 [153600/185639 (83%)]\tLoss: 0.553337\n",
      "Train Epoch: 8 [166400/185639 (90%)]\tLoss: 0.545960\n",
      "Train Epoch: 8 [179200/185639 (96%)]\tLoss: 0.459861\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4311/5198 (83%)\n",
      "\n",
      "Train Epoch: 9 [0/185639 (0%)]\tLoss: 0.390043\n",
      "Train Epoch: 9 [12800/185639 (7%)]\tLoss: 0.383039\n",
      "Train Epoch: 9 [25600/185639 (14%)]\tLoss: 0.472604\n",
      "Train Epoch: 9 [38400/185639 (21%)]\tLoss: 0.451125\n",
      "Train Epoch: 9 [51200/185639 (28%)]\tLoss: 0.316821\n",
      "Train Epoch: 9 [64000/185639 (34%)]\tLoss: 0.394737\n",
      "Train Epoch: 9 [76800/185639 (41%)]\tLoss: 0.395618\n",
      "Train Epoch: 9 [89600/185639 (48%)]\tLoss: 0.426698\n",
      "Train Epoch: 9 [102400/185639 (55%)]\tLoss: 0.470921\n",
      "Train Epoch: 9 [115200/185639 (62%)]\tLoss: 0.527671\n",
      "Train Epoch: 9 [128000/185639 (69%)]\tLoss: 0.385033\n",
      "Train Epoch: 9 [140800/185639 (76%)]\tLoss: 0.479199\n",
      "Train Epoch: 9 [153600/185639 (83%)]\tLoss: 0.484628\n",
      "Train Epoch: 9 [166400/185639 (90%)]\tLoss: 0.446005\n",
      "Train Epoch: 9 [179200/185639 (96%)]\tLoss: 0.454153\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4333/5198 (83%)\n",
      "\n",
      "Train Epoch: 10 [0/185639 (0%)]\tLoss: 0.595945\n",
      "Train Epoch: 10 [12800/185639 (7%)]\tLoss: 0.398832\n",
      "Train Epoch: 10 [25600/185639 (14%)]\tLoss: 0.454086\n",
      "Train Epoch: 10 [38400/185639 (21%)]\tLoss: 0.268268\n",
      "Train Epoch: 10 [51200/185639 (28%)]\tLoss: 0.373565\n",
      "Train Epoch: 10 [64000/185639 (34%)]\tLoss: 0.419435\n",
      "Train Epoch: 10 [76800/185639 (41%)]\tLoss: 0.371354\n",
      "Train Epoch: 10 [89600/185639 (48%)]\tLoss: 0.459325\n",
      "Train Epoch: 10 [102400/185639 (55%)]\tLoss: 0.437790\n",
      "Train Epoch: 10 [115200/185639 (62%)]\tLoss: 0.484675\n",
      "Train Epoch: 10 [128000/185639 (69%)]\tLoss: 0.356611\n",
      "Train Epoch: 10 [140800/185639 (76%)]\tLoss: 0.341313\n",
      "Train Epoch: 10 [153600/185639 (83%)]\tLoss: 0.510089\n",
      "Train Epoch: 10 [166400/185639 (90%)]\tLoss: 0.446274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [179200/185639 (96%)]\tLoss: 0.493275\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4353/5198 (84%)\n",
      "\n",
      "Train Epoch: 11 [0/185639 (0%)]\tLoss: 0.380724\n",
      "Train Epoch: 11 [12800/185639 (7%)]\tLoss: 0.347489\n",
      "Train Epoch: 11 [25600/185639 (14%)]\tLoss: 0.383677\n",
      "Train Epoch: 11 [38400/185639 (21%)]\tLoss: 0.364288\n",
      "Train Epoch: 11 [51200/185639 (28%)]\tLoss: 0.418152\n",
      "Train Epoch: 11 [64000/185639 (34%)]\tLoss: 0.337786\n",
      "Train Epoch: 11 [76800/185639 (41%)]\tLoss: 0.310047\n",
      "Train Epoch: 11 [89600/185639 (48%)]\tLoss: 0.347871\n",
      "Train Epoch: 11 [102400/185639 (55%)]\tLoss: 0.421650\n",
      "Train Epoch: 11 [115200/185639 (62%)]\tLoss: 0.434909\n",
      "Train Epoch: 11 [128000/185639 (69%)]\tLoss: 0.416854\n",
      "Train Epoch: 11 [140800/185639 (76%)]\tLoss: 0.538440\n",
      "Train Epoch: 11 [153600/185639 (83%)]\tLoss: 0.356966\n",
      "Train Epoch: 11 [166400/185639 (90%)]\tLoss: 0.551506\n",
      "Train Epoch: 11 [179200/185639 (96%)]\tLoss: 0.508393\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4355/5198 (84%)\n",
      "\n",
      "Train Epoch: 12 [0/185639 (0%)]\tLoss: 0.301509\n",
      "Train Epoch: 12 [12800/185639 (7%)]\tLoss: 0.293140\n",
      "Train Epoch: 12 [25600/185639 (14%)]\tLoss: 0.213905\n",
      "Train Epoch: 12 [38400/185639 (21%)]\tLoss: 0.232539\n",
      "Train Epoch: 12 [51200/185639 (28%)]\tLoss: 0.301886\n",
      "Train Epoch: 12 [64000/185639 (34%)]\tLoss: 0.288359\n",
      "Train Epoch: 12 [76800/185639 (41%)]\tLoss: 0.245867\n",
      "Train Epoch: 12 [89600/185639 (48%)]\tLoss: 0.429963\n",
      "Train Epoch: 12 [102400/185639 (55%)]\tLoss: 0.295058\n",
      "Train Epoch: 12 [115200/185639 (62%)]\tLoss: 0.301576\n",
      "Train Epoch: 12 [128000/185639 (69%)]\tLoss: 0.264965\n",
      "Train Epoch: 12 [140800/185639 (76%)]\tLoss: 0.388801\n",
      "Train Epoch: 12 [153600/185639 (83%)]\tLoss: 0.432167\n",
      "Train Epoch: 12 [166400/185639 (90%)]\tLoss: 0.357457\n",
      "Train Epoch: 12 [179200/185639 (96%)]\tLoss: 0.288969\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4333/5198 (83%)\n",
      "\n",
      "Train Epoch: 13 [0/185639 (0%)]\tLoss: 0.246517\n",
      "Train Epoch: 13 [12800/185639 (7%)]\tLoss: 0.333822\n",
      "Train Epoch: 13 [25600/185639 (14%)]\tLoss: 0.379324\n",
      "Train Epoch: 13 [38400/185639 (21%)]\tLoss: 0.231566\n",
      "Train Epoch: 13 [51200/185639 (28%)]\tLoss: 0.250459\n",
      "Train Epoch: 13 [64000/185639 (34%)]\tLoss: 0.243826\n",
      "Train Epoch: 13 [76800/185639 (41%)]\tLoss: 0.337210\n",
      "Train Epoch: 13 [89600/185639 (48%)]\tLoss: 0.249181\n",
      "Train Epoch: 13 [102400/185639 (55%)]\tLoss: 0.201103\n",
      "Train Epoch: 13 [115200/185639 (62%)]\tLoss: 0.304840\n",
      "Train Epoch: 13 [128000/185639 (69%)]\tLoss: 0.240800\n",
      "Train Epoch: 13 [140800/185639 (76%)]\tLoss: 0.293805\n",
      "Train Epoch: 13 [153600/185639 (83%)]\tLoss: 0.562797\n",
      "Train Epoch: 13 [166400/185639 (90%)]\tLoss: 0.299052\n",
      "Train Epoch: 13 [179200/185639 (96%)]\tLoss: 0.403004\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4356/5198 (84%)\n",
      "\n",
      "Train Epoch: 14 [0/185639 (0%)]\tLoss: 0.228019\n",
      "Train Epoch: 14 [12800/185639 (7%)]\tLoss: 0.198920\n",
      "Train Epoch: 14 [25600/185639 (14%)]\tLoss: 0.262769\n",
      "Train Epoch: 14 [38400/185639 (21%)]\tLoss: 0.217133\n",
      "Train Epoch: 14 [51200/185639 (28%)]\tLoss: 0.339035\n",
      "Train Epoch: 14 [64000/185639 (34%)]\tLoss: 0.254230\n",
      "Train Epoch: 14 [76800/185639 (41%)]\tLoss: 0.239915\n",
      "Train Epoch: 14 [89600/185639 (48%)]\tLoss: 0.352037\n",
      "Train Epoch: 14 [102400/185639 (55%)]\tLoss: 0.444144\n",
      "Train Epoch: 14 [115200/185639 (62%)]\tLoss: 0.264818\n",
      "Train Epoch: 14 [128000/185639 (69%)]\tLoss: 0.302191\n",
      "Train Epoch: 14 [140800/185639 (76%)]\tLoss: 0.244230\n",
      "Train Epoch: 14 [153600/185639 (83%)]\tLoss: 0.292573\n",
      "Train Epoch: 14 [166400/185639 (90%)]\tLoss: 0.332461\n",
      "Train Epoch: 14 [179200/185639 (96%)]\tLoss: 0.274705\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4371/5198 (84%)\n",
      "\n",
      "Train Epoch: 15 [0/185639 (0%)]\tLoss: 0.191497\n",
      "Train Epoch: 15 [12800/185639 (7%)]\tLoss: 0.206006\n",
      "Train Epoch: 15 [25600/185639 (14%)]\tLoss: 0.220545\n",
      "Train Epoch: 15 [38400/185639 (21%)]\tLoss: 0.248662\n",
      "Train Epoch: 15 [51200/185639 (28%)]\tLoss: 0.196351\n",
      "Train Epoch: 15 [64000/185639 (34%)]\tLoss: 0.258454\n",
      "Train Epoch: 15 [76800/185639 (41%)]\tLoss: 0.230172\n",
      "Train Epoch: 15 [89600/185639 (48%)]\tLoss: 0.212212\n",
      "Train Epoch: 15 [102400/185639 (55%)]\tLoss: 0.223192\n",
      "Train Epoch: 15 [115200/185639 (62%)]\tLoss: 0.195694\n",
      "Train Epoch: 15 [128000/185639 (69%)]\tLoss: 0.253129\n",
      "Train Epoch: 15 [140800/185639 (76%)]\tLoss: 0.347857\n",
      "Train Epoch: 15 [153600/185639 (83%)]\tLoss: 0.248274\n",
      "Train Epoch: 15 [166400/185639 (90%)]\tLoss: 0.203665\n",
      "Train Epoch: 15 [179200/185639 (96%)]\tLoss: 0.288226\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4376/5198 (84%)\n",
      "\n",
      "Train Epoch: 16 [0/185639 (0%)]\tLoss: 0.272162\n",
      "Train Epoch: 16 [12800/185639 (7%)]\tLoss: 0.274269\n",
      "Train Epoch: 16 [25600/185639 (14%)]\tLoss: 0.179335\n",
      "Train Epoch: 16 [38400/185639 (21%)]\tLoss: 0.207447\n",
      "Train Epoch: 16 [51200/185639 (28%)]\tLoss: 0.254701\n",
      "Train Epoch: 16 [64000/185639 (34%)]\tLoss: 0.210382\n",
      "Train Epoch: 16 [76800/185639 (41%)]\tLoss: 0.278212\n",
      "Train Epoch: 16 [89600/185639 (48%)]\tLoss: 0.258805\n",
      "Train Epoch: 16 [102400/185639 (55%)]\tLoss: 0.490489\n",
      "Train Epoch: 16 [115200/185639 (62%)]\tLoss: 0.243344\n",
      "Train Epoch: 16 [128000/185639 (69%)]\tLoss: 0.228832\n",
      "Train Epoch: 16 [140800/185639 (76%)]\tLoss: 0.271715\n",
      "Train Epoch: 16 [153600/185639 (83%)]\tLoss: 0.180035\n",
      "Train Epoch: 16 [166400/185639 (90%)]\tLoss: 0.358453\n",
      "Train Epoch: 16 [179200/185639 (96%)]\tLoss: 0.320560\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4432/5198 (85%)\n",
      "\n",
      "Train Epoch: 17 [0/185639 (0%)]\tLoss: 0.209365\n",
      "Train Epoch: 17 [12800/185639 (7%)]\tLoss: 0.247362\n",
      "Train Epoch: 17 [25600/185639 (14%)]\tLoss: 0.274050\n",
      "Train Epoch: 17 [38400/185639 (21%)]\tLoss: 0.190216\n",
      "Train Epoch: 17 [51200/185639 (28%)]\tLoss: 0.204652\n",
      "Train Epoch: 17 [64000/185639 (34%)]\tLoss: 0.187875\n",
      "Train Epoch: 17 [76800/185639 (41%)]\tLoss: 0.264294\n",
      "Train Epoch: 17 [89600/185639 (48%)]\tLoss: 0.273952\n",
      "Train Epoch: 17 [102400/185639 (55%)]\tLoss: 0.194397\n",
      "Train Epoch: 17 [115200/185639 (62%)]\tLoss: 0.249676\n",
      "Train Epoch: 17 [128000/185639 (69%)]\tLoss: 0.137786\n",
      "Train Epoch: 17 [140800/185639 (76%)]\tLoss: 0.299816\n",
      "Train Epoch: 17 [153600/185639 (83%)]\tLoss: 0.256203\n",
      "Train Epoch: 17 [166400/185639 (90%)]\tLoss: 0.250465\n",
      "Train Epoch: 17 [179200/185639 (96%)]\tLoss: 0.282429\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4373/5198 (84%)\n",
      "\n",
      "Train Epoch: 18 [0/185639 (0%)]\tLoss: 0.212494\n",
      "Train Epoch: 18 [12800/185639 (7%)]\tLoss: 0.183466\n",
      "Train Epoch: 18 [25600/185639 (14%)]\tLoss: 0.255604\n",
      "Train Epoch: 18 [38400/185639 (21%)]\tLoss: 0.221334\n",
      "Train Epoch: 18 [51200/185639 (28%)]\tLoss: 0.212034\n",
      "Train Epoch: 18 [64000/185639 (34%)]\tLoss: 0.119352\n",
      "Train Epoch: 18 [76800/185639 (41%)]\tLoss: 0.155564\n",
      "Train Epoch: 18 [89600/185639 (48%)]\tLoss: 0.229979\n",
      "Train Epoch: 18 [102400/185639 (55%)]\tLoss: 0.213678\n",
      "Train Epoch: 18 [115200/185639 (62%)]\tLoss: 0.194751\n",
      "Train Epoch: 18 [128000/185639 (69%)]\tLoss: 0.154467\n",
      "Train Epoch: 18 [140800/185639 (76%)]\tLoss: 0.249727\n",
      "Train Epoch: 18 [153600/185639 (83%)]\tLoss: 0.197774\n",
      "Train Epoch: 18 [166400/185639 (90%)]\tLoss: 0.247471\n",
      "Train Epoch: 18 [179200/185639 (96%)]\tLoss: 0.324401\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4426/5198 (85%)\n",
      "\n",
      "Train Epoch: 19 [0/185639 (0%)]\tLoss: 0.125607\n",
      "Train Epoch: 19 [12800/185639 (7%)]\tLoss: 0.301026\n",
      "Train Epoch: 19 [25600/185639 (14%)]\tLoss: 0.179307\n",
      "Train Epoch: 19 [38400/185639 (21%)]\tLoss: 0.247343\n",
      "Train Epoch: 19 [51200/185639 (28%)]\tLoss: 0.188504\n",
      "Train Epoch: 19 [64000/185639 (34%)]\tLoss: 0.195507\n",
      "Train Epoch: 19 [76800/185639 (41%)]\tLoss: 0.229371\n",
      "Train Epoch: 19 [89600/185639 (48%)]\tLoss: 0.183452\n",
      "Train Epoch: 19 [102400/185639 (55%)]\tLoss: 0.163870\n",
      "Train Epoch: 19 [115200/185639 (62%)]\tLoss: 0.313870\n",
      "Train Epoch: 19 [128000/185639 (69%)]\tLoss: 0.199236\n",
      "Train Epoch: 19 [140800/185639 (76%)]\tLoss: 0.191656\n",
      "Train Epoch: 19 [153600/185639 (83%)]\tLoss: 0.267063\n",
      "Train Epoch: 19 [166400/185639 (90%)]\tLoss: 0.142034\n",
      "Train Epoch: 19 [179200/185639 (96%)]\tLoss: 0.134168\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4412/5198 (85%)\n",
      "\n",
      "Train Epoch: 20 [0/185639 (0%)]\tLoss: 0.107297\n",
      "Train Epoch: 20 [12800/185639 (7%)]\tLoss: 0.220921\n",
      "Train Epoch: 20 [25600/185639 (14%)]\tLoss: 0.169358\n",
      "Train Epoch: 20 [38400/185639 (21%)]\tLoss: 0.080812\n",
      "Train Epoch: 20 [51200/185639 (28%)]\tLoss: 0.207585\n",
      "Train Epoch: 20 [64000/185639 (34%)]\tLoss: 0.163603\n",
      "Train Epoch: 20 [76800/185639 (41%)]\tLoss: 0.182333\n",
      "Train Epoch: 20 [89600/185639 (48%)]\tLoss: 0.116663\n",
      "Train Epoch: 20 [102400/185639 (55%)]\tLoss: 0.107308\n",
      "Train Epoch: 20 [115200/185639 (62%)]\tLoss: 0.205735\n",
      "Train Epoch: 20 [128000/185639 (69%)]\tLoss: 0.147932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [140800/185639 (76%)]\tLoss: 0.160762\n",
      "Train Epoch: 20 [153600/185639 (83%)]\tLoss: 0.303727\n",
      "Train Epoch: 20 [166400/185639 (90%)]\tLoss: 0.146063\n",
      "Train Epoch: 20 [179200/185639 (96%)]\tLoss: 0.196134\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4371/5198 (84%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (20 epochs) : 637.104\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "n_epochs = 20\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#training model\n",
    "want_to_train = True\n",
    "want_new_model = True\n",
    "\n",
    "if want_new_model:\n",
    "    model = CharNetMaxout('ICDAR')\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "if want_to_train:\n",
    "    optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
    "    print(\"\\n Launching the training process...\")\n",
    "    model.exec(train_loader, test_loader, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the best CharNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharNetMaxout('ICDAR')\n",
    "model.load_state_dict(torch.load('./models/best/charnet_maxout/charnet_maxout_84.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8410927279722971, 17.060295609999997)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._test(test_loader, [], [])[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposing 2nd and 3rd layer of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4372/5198 (84%)\n",
      "\n",
      "\n",
      "Layer: 2\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0011, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010, Accuracy: 495/5198 (10%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 495/5198 (10%)\n",
      "\n",
      "Rank: 4, Approx error : 97.65 %,  Accuracy drop : 88.68 %, Speed-up : 47.00 %, Param reduction : 478.523\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0010, Accuracy: 495/5198 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 6.127635\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.947718\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.892861\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.656120\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.457332\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.606904\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.711314\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.422523\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.548550\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.517121\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.516821\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.414983\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.163995\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.307570\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.257718\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3724/5198 (72%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.446795\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.203216\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.230970\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.087269\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.278210\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.218395\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.265549\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.956226\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.159712\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.242814\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.069732\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.149622\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.097319\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.209994\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.961789\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3813/5198 (73%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.084\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3813/5198 (73%)\n",
      "\n",
      "After fine-tunning: Accuracy : 73.35513659099654 %\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0009, Accuracy: 925/5198 (18%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 925/5198 (18%)\n",
      "\n",
      "Rank: 8, Approx error : 96.65 %,  Accuracy drop : 78.84 %, Speed-up : 95.25 %, Param reduction : 239.262\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 925/5198 (18%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 5.258496\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.520632\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.507490\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.300058\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.328223\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.178075\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.332187\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.066737\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.189015\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.194839\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.995591\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.021754\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.888738\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.062362\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.978042\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 3959/5198 (76%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.083418\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.108206\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.167860\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.842236\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.008504\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.750223\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.822454\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.851735\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.914806\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.824455\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.094524\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.915640\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.834243\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.758077\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.027206\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4104/5198 (79%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.427\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4104/5198 (79%)\n",
      "\n",
      "After fine-tunning: Accuracy : 78.95344363216623 %\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "Rank: 16, Approx error : 94.98 %,  Accuracy drop : 46.80 %, Speed-up : 95.18 %, Param reduction : 119.631\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2326/5198 (45%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 3.031243\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.187703\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.282049\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.361946\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.067736\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.983992\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.095829\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.085007\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.702875\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.778793\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.858984\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.870215\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.906953\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.869144\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.947780\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4155/5198 (80%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.850838\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.912408\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.846710\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.732641\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.845835\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.703032\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.684510\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.859046\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.637257\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.781187\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.823682\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.684767\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.753905\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.775816\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.774698\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4208/5198 (81%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.409\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4208/5198 (81%)\n",
      "\n",
      "After fine-tunning: Accuracy : 80.95421315890728 %\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "Rank: 32, Approx error : 92.31 %,  Accuracy drop : 20.54 %, Speed-up : 95.17 %, Param reduction : 59.815\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3474/5198 (67%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.690107\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.238513\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.064879\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.097991\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.875944\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.766139\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.071685\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.758993\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.825133\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.877544\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.943822\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.623251\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.716161\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.833891\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.903037\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4221/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.567659\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.671193\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.580866\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.728556\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.597769\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.724988\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.615820\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.532903\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.654154\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.530385\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.663560\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.721730\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.684550\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.727284\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.779782\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4278/5198 (82%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.370\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4278/5198 (82%)\n",
      "\n",
      "After fine-tunning: Accuracy : 82.30088495575221 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "Rank: 64, Approx error : 88.62 %,  Accuracy drop : 6.63 %, Speed-up : 95.15 %, Param reduction : 29.908\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4082/5198 (79%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.069927\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.879356\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.671275\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.844460\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.722179\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.717999\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.696037\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.709005\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.630798\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.549513\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.764689\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.687181\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.653577\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.581637\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.642094\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.710557\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.574560\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.511789\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.427775\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.569801\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.509375\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.529268\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.540638\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.542959\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.761233\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.586390\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.456940\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.586039\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.523979\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.560783\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4300/5198 (83%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.907\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4300/5198 (83%)\n",
      "\n",
      "After fine-tunning: Accuracy : 82.72412466333205 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "Rank: 128, Approx error : 83.69 %,  Accuracy drop : 2.42 %, Speed-up : 95.28 %, Param reduction : 14.954\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4266/5198 (82%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.536986\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.820728\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.674485\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.630906\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.738992\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.852397\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.713593\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.915215\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.398638\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.665834\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.891496\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.728170\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.466837\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.480618\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.452563\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4235/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.499232\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.550294\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.604568\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.544846\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.650973\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.428112\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.606018\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.474777\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.371362\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.440855\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.575090\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.535085\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.598839\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.656154\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.513329\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4322/5198 (83%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 66.754\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4322/5198 (83%)\n",
      "\n",
      "After fine-tunning: Accuracy : 83.14736437091189 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "Rank: 256, Approx error : 75.95 %,  Accuracy drop : 1.10 %, Speed-up : 95.16 %, Param reduction : 7.477\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4324/5198 (83%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.443860\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.744348\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.704113\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.847238\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.547457\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.863533\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.578240\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.727053\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.761087\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.610253\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.014358\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.548968\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.632334\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.532433\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.722831\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4210/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.472385\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.572459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.395856\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.654820\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.647448\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.549973\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.513816\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.477088\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.375495\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.485961\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.507046\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.656108\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.545686\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.476511\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.628147\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4239/5198 (82%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 68.903\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4239/5198 (82%)\n",
      "\n",
      "After fine-tunning: Accuracy : 81.55059638322432 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "Rank: 512, Approx error : 63.31 %,  Accuracy drop : 0.57 %, Speed-up : 95.24 %, Param reduction : 3.738\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4347/5198 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.243264\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.923306\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.795712\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.745828\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.747481\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.955990\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.607274\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.522309\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.858664\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.507271\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.765924\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.439370\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.387228\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.600080\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.720540\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4165/5198 (80%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.597587\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.410097\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.385897\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.457349\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.445287\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.602119\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.611591\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.584583\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.367796\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.390068\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.752149\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.569164\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.666523\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.609910\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.654770\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4275/5198 (82%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 90.673\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4275/5198 (82%)\n",
      "\n",
      "After fine-tunning: Accuracy : 82.24317045017314 %\n",
      "\n",
      "Layer: 3\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "Rank: 4, Approx error : 99.04 %,  Accuracy drop : 72.69 %, Speed-up : 95.34 %, Param reduction : 630.154\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1194/5198 (23%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 3.270697\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.449231\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.429618\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.378717\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.245963\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.355427\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.136483\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.224829\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.162602\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.123986\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.282798\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.260916\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.210525\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.042062\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.249147\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3900/5198 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.200378\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.185274\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.039351\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.062083\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.949736\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.999713\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.121933\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.887783\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.965938\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.952016\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.988500\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.009142\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.056038\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.844520\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.969838\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4011/5198 (77%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 57.406\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4011/5198 (77%)\n",
      "\n",
      "After fine-tunning: Accuracy : 77.16429395921509 %\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "Rank: 8, Approx error : 98.23 %,  Accuracy drop : 49.77 %, Speed-up : 95.43 %, Param reduction : 315.077\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 2196/5198 (42%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 2.731946\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.291408\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.249646\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.997033\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.840993\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.098768\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.054174\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.010315\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.888677\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.992280\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.137435\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.047031\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.874517\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.937757\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.033547\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4045/5198 (78%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.024391\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.986325\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.094322\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.008614\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.985000\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.996568\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.833752\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.833363\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.853154\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.829545\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.823646\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.945156\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.847997\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.931535\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.698822\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4145/5198 (80%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 57.234\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4145/5198 (80%)\n",
      "\n",
      "After fine-tunning: Accuracy : 79.74220854174683 %\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "Rank: 16, Approx error : 96.89 %,  Accuracy drop : 24.84 %, Speed-up : 95.28 %, Param reduction : 157.538\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3286/5198 (63%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.929399\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.147966\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.982068\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.059492\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.987915\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.927265\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.896933\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.980465\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.029687\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.925285\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.915464\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.940438\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.853744\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.928771\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.747156\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4130/5198 (79%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.959823\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.749010\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.774916\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.769051\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.688965\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.806494\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.717369\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.664941\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.769579\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.879137\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.785136\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.635305\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.744056\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.702612\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.646351\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4203/5198 (81%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 60.466\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4203/5198 (81%)\n",
      "\n",
      "After fine-tunning: Accuracy : 80.8580223162755 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "Rank: 32, Approx error : 94.81 %,  Accuracy drop : 9.61 %, Speed-up : 95.40 %, Param reduction : 78.769\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3952/5198 (76%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.139068\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.951814\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.826803\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.862625\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.900740\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.775850\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.976974\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.114866\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.991595\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.919230\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.914881\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.883895\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.924049\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.976011\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.049571\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4223/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.780793\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.756737\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.795414\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.937678\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.790944\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.829937\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.707224\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.783031\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.624105\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.656612\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.716222\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.670749\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.667867\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.880533\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.691207\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4268/5198 (82%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 57.505\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4268/5198 (82%)\n",
      "\n",
      "After fine-tunning: Accuracy : 82.10850327048865 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "Rank: 64, Approx error : 91.78 %,  Accuracy drop : 2.36 %, Speed-up : 95.41 %, Param reduction : 39.385\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.697282\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.951942\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.647784\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.939647\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.678876\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.895300\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.999071\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.949230\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.735619\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.901560\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.881922\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.834743\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.699567\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.683149\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.787081\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4245/5198 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.887543\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.659739\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.845949\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.632521\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.722277\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.608008\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.896806\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.565407\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.648802\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.714380\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.583322\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.626159\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.651368\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.851494\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.863899\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4350/5198 (84%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 57.682\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4350/5198 (84%)\n",
      "\n",
      "After fine-tunning: Accuracy : 83.68603308964987 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "Rank: 128, Approx error : 87.34 %,  Accuracy drop : 0.30 %, Speed-up : 95.46 %, Param reduction : 19.692\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4359/5198 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.674137\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.120304\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.829727\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.047758\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.007637\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.711526\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.602762\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.825125\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.712160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.815782\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.748099\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.839273\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.751311\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.645402\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.568669\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4256/5198 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.831410\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.720828\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.561259\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.829816\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.626618\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.651444\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.717722\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.739654\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.929879\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.786410\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.028283\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.656488\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.720164\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.637878\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.746648\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4368/5198 (84%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 59.831\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4368/5198 (84%)\n",
      "\n",
      "After fine-tunning: Accuracy : 84.03232012312428 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "Rank: 256, Approx error : 80.69 %,  Accuracy drop : -0.27 %, Speed-up : 95.35 %, Param reduction : 9.846\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4384/5198 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.442316\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.040300\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.000269\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.154826\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 0.809101\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.085506\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.070639\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 0.905456\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.799192\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 0.867971\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.992269\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.772209\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.770411\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.894316\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.879311\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4210/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.814206\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.928785\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.848788\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.666053\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.629250\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.562694\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.653296\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.667310\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.695719\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.704040\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.880107\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.750445\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.749106\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.547442\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.700311\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4334/5198 (83%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 59.415\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4334/5198 (83%)\n",
      "\n",
      "After fine-tunning: Accuracy : 83.37822239322816 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "Rank: 512, Approx error : 70.33 %,  Accuracy drop : -0.62 %, Speed-up : 95.41 %, Param reduction : 4.923\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4399/5198 (85%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.387123\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 0.859209\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.939545\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.776576\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.042019\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.312984\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 0.930478\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.014864\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.787898\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.227210\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 0.938126\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.134096\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.700110\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.950171\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.883486\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4214/5198 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.899242\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 0.783510\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.879725\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.805962\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.909939\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 0.701032\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.828831\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.665135\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.747803\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.584063\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.823687\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.786044\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.837135\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.698271\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.764218\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 58.937\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4269/5198 (82%)\n",
      "\n",
      "After fine-tunning: Accuracy : 82.127741439015 %\n",
      "\n",
      "Layer: 0\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 138/5198 (3%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "Rank: 4, Approx error : 98.56 %,  Accuracy drop : 96.84 %, Speed-up : 95.35 %, Param reduction : 594.051\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 138/5198 (3%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 4.349240\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 2.683238\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 2.642720\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 2.291277\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 2.357977\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 2.264372\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 2.105504\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 2.153928\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 2.170449\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.810203\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.972099\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.959839\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.982271\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.859426\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.852762\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 2828/5198 (54%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.668627\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.730394\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.818825\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.699832\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.932886\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.642513\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.830004\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.899735\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.644108\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.920536\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.817624\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.573243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.754332\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.815551\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.510472\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3130/5198 (60%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 60.471\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3130/5198 (60%)\n",
      "\n",
      "After fine-tunning: Accuracy : 60.21546748749519 %\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 463/5198 (9%)\n",
      "\n",
      "Rank: 8, Approx error : 97.70 %,  Accuracy drop : 89.41 %, Speed-up : 95.23 %, Param reduction : 297.026\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 463/5198 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 4.274139\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 2.275059\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.971255\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.920069\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.717994\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.535913\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.660321\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.525953\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.644073\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.677639\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.586005\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.536395\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.626822\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.717355\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.527659\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3311/5198 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.504269\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.558533\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.259620\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.442818\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.252480\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.482261\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.518396\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.125319\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.305070\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.398546\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.377407\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.296123\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.336868\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.164324\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.324604\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3636/5198 (70%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 60.124\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3636/5198 (70%)\n",
      "\n",
      "After fine-tunning: Accuracy : 69.94998076183148 %\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0007, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "Rank: 16, Approx error : 96.25 %,  Accuracy drop : 69.99 %, Speed-up : 95.40 %, Param reduction : 148.513\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0006, Accuracy: 1312/5198 (25%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 3.609693\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.689099\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.700734\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.599745\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.492785\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.359336\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.244626\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.329452\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.585061\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.321609\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.219139\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.330827\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.461532\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.391347\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.160020\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3798/5198 (73%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.128592\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.280464\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.080070\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.418624\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.213380\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.138690\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.160997\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.271201\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.052945\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.163691\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.104597\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.050927\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.136132\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.018318\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.033241\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3924/5198 (75%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 59.390\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3924/5198 (75%)\n",
      "\n",
      "After fine-tunning: Accuracy : 75.49057329742209 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "Rank: 32, Approx error : 93.93 %,  Accuracy drop : 25.59 %, Speed-up : 95.25 %, Param reduction : 74.256\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3253/5198 (63%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.874733\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.253905\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.206901\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.243881\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.195286\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.906607\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.309590\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.149562\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.211890\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.300782\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.316470\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.945320\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.103803\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.111405\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.962924\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4019/5198 (77%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.978593\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.069329\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.046032\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.087731\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.812417\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.091359\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.874950\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.085886\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.057253\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.961297\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.938468\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.976123\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.079736\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.914232\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.019013\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4096/5198 (79%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 59.622\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4096/5198 (79%)\n",
      "\n",
      "After fine-tunning: Accuracy : 78.79953828395537 %\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "Rank: 64, Approx error : 90.80 %,  Accuracy drop : 12.76 %, Speed-up : 95.38 %, Param reduction : 37.128\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3814/5198 (73%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 1.194570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.263109\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 0.983020\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 0.929873\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.125737\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.135103\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.036846\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.010753\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 0.995670\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.276456\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.194084\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.997278\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.076310\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 0.911890\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.041819\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4135/5198 (80%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.835070\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.005423\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.024412\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 0.976040\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.863599\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.079946\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.869733\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 0.800780\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.021799\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.080238\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.900522\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 0.792393\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.901425\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.843303\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.968781\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4160/5198 (80%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 59.901\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4160/5198 (80%)\n",
      "\n",
      "After fine-tunning: Accuracy : 80.03078106964216 %\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "Rank: 128, Approx error : 86.10 %,  Accuracy drop : 3.48 %, Speed-up : 95.29 %, Param reduction : 18.564\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4220/5198 (81%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.860340\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.531291\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.283640\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.312841\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.281794\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 0.980380\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.283556\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.468722\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.287112\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.224338\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.293890\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 0.928362\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 0.997078\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.061401\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 0.983692\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4108/5198 (79%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 0.875007\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.165998\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.029204\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.107372\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 0.871073\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.039697\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 0.772979\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.048589\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.934388\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.211772\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.814405\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.038501\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 0.976607\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.049503\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.868525\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4168/5198 (80%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 60.274\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4168/5198 (80%)\n",
      "\n",
      "After fine-tunning: Accuracy : 80.18468641785303 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "Rank: 256, Approx error : 79.12 %,  Accuracy drop : 0.82 %, Speed-up : 95.33 %, Param reduction : 9.282\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4336/5198 (83%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.414048\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 1.711361\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 1.788341\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 1.580410\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 1.739581\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 2.013333\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 1.701230\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 1.446917\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.866856\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 1.579627\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.500673\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.516744\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.271652\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.396078\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 1.433096\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3972/5198 (76%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 1.278392\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.107503\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 0.734322\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.083142\n",
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.103547\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.156220\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.353446\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.070921\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 0.981557\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 0.831241\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 0.928320\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.079759\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.095736\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 0.973055\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 0.837988\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4104/5198 (79%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 62.251\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4104/5198 (79%)\n",
      "\n",
      "After fine-tunning: Accuracy : 78.95344363216623 %\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "Rank: 512, Approx error : 67.98 %,  Accuracy drop : 0.05 %, Speed-up : 95.15 %, Param reduction : 4.641\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4370/5198 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/185639 (0%)]\tLoss: 0.424858\n",
      "Train Epoch: 1 [12800/185639 (7%)]\tLoss: 2.592038\n",
      "Train Epoch: 1 [25600/185639 (14%)]\tLoss: 2.550343\n",
      "Train Epoch: 1 [38400/185639 (21%)]\tLoss: 2.907419\n",
      "Train Epoch: 1 [51200/185639 (28%)]\tLoss: 2.396231\n",
      "Train Epoch: 1 [64000/185639 (34%)]\tLoss: 1.766918\n",
      "Train Epoch: 1 [76800/185639 (41%)]\tLoss: 2.699000\n",
      "Train Epoch: 1 [89600/185639 (48%)]\tLoss: 2.769240\n",
      "Train Epoch: 1 [102400/185639 (55%)]\tLoss: 1.758411\n",
      "Train Epoch: 1 [115200/185639 (62%)]\tLoss: 2.104622\n",
      "Train Epoch: 1 [128000/185639 (69%)]\tLoss: 1.943951\n",
      "Train Epoch: 1 [140800/185639 (76%)]\tLoss: 1.539114\n",
      "Train Epoch: 1 [153600/185639 (83%)]\tLoss: 1.836046\n",
      "Train Epoch: 1 [166400/185639 (90%)]\tLoss: 1.294459\n",
      "Train Epoch: 1 [179200/185639 (96%)]\tLoss: 2.016561\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 3934/5198 (76%)\n",
      "\n",
      "Train Epoch: 2 [0/185639 (0%)]\tLoss: 2.024844\n",
      "Train Epoch: 2 [12800/185639 (7%)]\tLoss: 1.131189\n",
      "Train Epoch: 2 [25600/185639 (14%)]\tLoss: 1.322697\n",
      "Train Epoch: 2 [38400/185639 (21%)]\tLoss: 1.567743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [51200/185639 (28%)]\tLoss: 1.752082\n",
      "Train Epoch: 2 [64000/185639 (34%)]\tLoss: 1.405688\n",
      "Train Epoch: 2 [76800/185639 (41%)]\tLoss: 1.444723\n",
      "Train Epoch: 2 [89600/185639 (48%)]\tLoss: 1.348769\n",
      "Train Epoch: 2 [102400/185639 (55%)]\tLoss: 1.406034\n",
      "Train Epoch: 2 [115200/185639 (62%)]\tLoss: 1.320846\n",
      "Train Epoch: 2 [128000/185639 (69%)]\tLoss: 1.224337\n",
      "Train Epoch: 2 [140800/185639 (76%)]\tLoss: 1.212882\n",
      "Train Epoch: 2 [153600/185639 (83%)]\tLoss: 1.279458\n",
      "Train Epoch: 2 [166400/185639 (90%)]\tLoss: 1.175914\n",
      "Train Epoch: 2 [179200/185639 (96%)]\tLoss: 1.107611\n",
      "\n",
      "Test set: Avg. loss: 0.0001, Accuracy: 4070/5198 (78%)\n",
      "\n",
      "\n",
      " Total Time Elapsed (2 epochs) : 92.856\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 4070/5198 (78%)\n",
      "\n",
      "After fine-tunning: Accuracy : 78.2993459022701 %\n"
     ]
    }
   ],
   "source": [
    "ranks_to_decomp = [4,8,16,32,64,128,256,512]\n",
    "convs = [2,3,0]\n",
    "adress = './models/best/charnet_maxout/charnet_maxout_84.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 2\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "tot_time = 0\n",
    "for iter in range(8):\n",
    "    old_acc, old_computation_time = model._test(test_loader, [], [], device)[1:]\n",
    "    tot_time += old_computation_time\n",
    "    \n",
    "old_computation_time = tot_time / 8\n",
    "approximation_list, dim_drop_list, acc_list, computation_list, acc_ft_list = [], [], [], [], []\n",
    "\n",
    "for conv in convs:\n",
    "    print(\"\\nLayer: {}\\n\".format(conv))\n",
    "    \n",
    "    app, dim, acc, comp, acc_ft = [], [], [], [], []\n",
    "    for rank in ranks_to_decomp:\n",
    "\n",
    "        model.to(\"cpu\")\n",
    "\n",
    "        approximation, dim_drop = model.decompose(n_layer=conv, rank=rank)\n",
    "        model.to(device)\n",
    "        \n",
    "        tot_time = 0\n",
    "        for iter in range(8):\n",
    "            new_acc, new_computation_time = model._test(test_loader, [], [], device)[1:]\n",
    "            tot_time += new_computation_time\n",
    "        new_computation_time = tot_time / 8\n",
    "        \n",
    "        accuracy_drop = (old_acc - new_acc) / old_acc\n",
    "        speed_up = (old_computation_time - new_computation_time) / old_computation_time\n",
    "\n",
    "        app.append(100 * approximation)\n",
    "        dim.append(dim_drop)\n",
    "        acc.append(100 * accuracy_drop)\n",
    "        comp.append(100 * speed_up)\n",
    "\n",
    "        print(\"Rank: {}, Approx error : {:.2f} %,  Accuracy drop : {:.2f} %, \"\n",
    "              \"Speed-up : {:.2f} %, Param reduction : {:.3f}\\n\".format(rank, 100 * approximation,\n",
    "                                                                      100 * accuracy_drop, 100 * speed_up,\n",
    "                                                                      dim_drop))\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr= 0.0001)\n",
    "        model.exec(train_loader, test_loader, optimizer, n_epochs, device)\n",
    "\n",
    "\n",
    "        ft_acc, ft_computation_time = model._test(test_loader, [], [], device)[1:]\n",
    "        accuracy_drop = (old_acc - ft_acc) / old_acc\n",
    "        acc_ft.append(100 * accuracy_drop)\n",
    "\n",
    "        print(\"After fine-tunning: Accuracy : {} %\".format(ft_acc*100))\n",
    "\n",
    "        model = CharNetMaxout(\"ICDAR\")\n",
    "\n",
    "        model.load_state_dict(torch.load(adress))\n",
    "\n",
    "    # Add the performance indicators values to the global lists\n",
    "    approximation_list.append(app)\n",
    "    dim_drop_list.append(dim)\n",
    "    acc_list.append(acc)\n",
    "    acc_ft_list.append(acc_ft)\n",
    "    computation_list.append(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output reworking for a better plotting\n",
    "for i,a in enumerate(acc_list):\n",
    "    for j, b in enumerate(a):\n",
    "        if b < 0.01:\n",
    "            acc_list[i][j]=0.01\n",
    "            \n",
    "for i,a in enumerate(acc_ft_list):\n",
    "    for j, b in enumerate(a):\n",
    "        if b < 0.01:\n",
    "            acc_ft_list[i][j]=0.01\n",
    "            \n",
    "for i,a in enumerate(computation_list):\n",
    "    for j, b in enumerate(a):\n",
    "        if b < 0.01:\n",
    "            computation_list[i][j]=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = ranks_to_decomp\n",
    "\n",
    "formatter = ticker.ScalarFormatter()\n",
    "formatter.set_scientific(False)\n",
    "\n",
    "x_labels = [4, 16, 64,  256]\n",
    "err_ticks = [100, 80, 60 ,40, 20]print(\"this is batch_idx\") #\n",
    "            print(batch_idx) #\n",
    "err_ticks1 = [100, 80, 60, 40, 30, 20 ,0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, nrows=4, figsize=(16, 14))\n",
    "ax = ax.flatten()\n",
    "\n",
    "plt.suptitle('CharNet Maxout', fontsize=24)\n",
    "\n",
    "ax[0].plot(x, approximation_list[0], marker='o', mfc='none',  c='#3838D0')\n",
    "ax[0].set_ylabel('Approximation Error (%)')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[0].set_xticks(x_labels)\n",
    "ax[0].set_yticks(err_ticks)\n",
    "\n",
    "\n",
    "ax[1].plot(x, approximation_list[1], marker='o', mfc='none', c='#3838D0')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[1].set_xticks(x_labels)\n",
    "ax[1].set_yticks(err_ticks)\n",
    "\n",
    "\n",
    "ax[2].plot(x, approximation_list[2], marker='o', mfc='none', c='#3838D0')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[2].set_xticks(x_labels)\n",
    "ax[2].set_yticks(err_ticks)\n",
    "\n",
    "\n",
    "ax[3].plot(x, acc_list[0], marker='o', mfc='none', c='red')\n",
    "ax[3].plot(x, acc_ft_list[0], 'r--', marker='o', mfc='none')\n",
    "ax[3].set_ylabel('Accuracy Drop (%)')\n",
    "ax[3].set_xscale('log')\n",
    "ax[3].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[3].set_xticks(x_labels)\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_yticks([100, 10, 1, 0.1, 0.01])print(\"this is batch_idx\") #\n",
    "            print(batch_idx) #\n",
    "ax[3].yaxis.set_major_formatter(formatter)\n",
    "ax[3].set_ylim([0.01,100])\n",
    "\n",
    "\n",
    "ax[4].plot(x, acc_list[1], marker='o', mfc='none', c='red')\n",
    "ax[4].plot(x, acc_ft_list[1], 'r--', marker='o', mfc='none')\n",
    "ax[4].set_xscale('log')\n",
    "ax[4].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[4].set_xticks(x_labels)\n",
    "ax[4].set_yscale('log')\n",
    "ax[4].set_yticks([100, 10, 1, 0.1, 0.01])\n",
    "ax[4].yaxis.set_major_formatter(formatter)\n",
    "ax[4].set_ylim([0.01,100])\n",
    "\n",
    "\n",
    "ax[5].plot(x, acc_list[2], marker='o', mfc='none', c='red')\n",
    "ax[5].plot(x, acc_ft_list[2], 'r--', marker='o', mfc='none')\n",
    "ax[5].set_xscale('log')\n",
    "ax[5].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[5].set_xticks(x_labels)\n",
    "ax[5].set_yscale('log')\n",
    "ax[5].set_yticks([100, 10, 1, 0.1, 0.01])\n",
    "ax[5].yaxis.set_major_formatter(formatter)\n",
    "ax[5].set_ylim([0.01,100])\n",
    "\n",
    "\n",
    "ax[6].plot(x, computation_list[0], marker='o', mfc='none', c='green')\n",
    "ax[6].set_ylabel('Speed-up (%)')\n",
    "ax[6].set_xscale('log')\n",
    "ax[6].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[6].set_xticks(x_labels)\n",
    "ax[6].set_yscale('linear')\n",
    "ax[6].yaxis.set_major_formatter(formatter)\n",
    "ax[6].set_ylim([0,100])\n",
    "\n",
    "\n",
    "ax[7].plot(x, computation_list[1], marker='o', mfc='none', c='green')\n",
    "ax[7].set_xscale('log')\n",
    "ax[7].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[7].set_xticks(x_labels)\n",
    "ax[7].set_yscale('linear')\n",
    "ax[7].yaxis.set_major_formatter(formatter)\n",
    "ax[7].set_ylim([0,100])\n",
    "\n",
    "\n",
    "ax[8].plot(x, computation_list[2], marker='o', mfc='none', c='green')\n",
    "ax[8].set_yscale('linear')\n",
    "ax[8].yaxis.set_major_formatter(formatter)\n",
    "ax[8].set_xscale('log')\n",
    "ax[8].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[8].set_xticks(x_labels)\n",
    "ax[8].set_ylim([0,100])\n",
    "\n",
    "\n",
    "ax[9].plot(x, dim_drop_list[0], marker='o', mfc='none', c='#2C89D0')\n",
    "ax[9].set_ylabel('Parameters reduction (x)')\n",
    "ax[9].set_xlabel('CharNet conv2')\n",
    "ax[9].set_xscale('log')\n",
    "ax[9].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[9].set_xticks(x_labels)\n",
    "ax[9].set_ylim([0,600])\n",
    "\n",
    "\n",
    "ax[10].plot(x, dim_drop_list[1], marker='o', mfc='none', c='#2C89D0')\n",
    "ax[10].set_xlabel('CharNet conv3')\n",
    "ax[10].set_xscale('log')\n",
    "ax[10].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[10].set_xticks(x_labels)\n",
    "ax[10].set_ylim([0,600])\n",
    "\n",
    "\n",
    "ax[11].plot(x, dim_drop_list[2], marker='o', mfc='none', c='#2C89D0')\n",
    "ax[11].set_xlabel('CharNet conv2 & conv3')\n",
    "ax[11].set_xscale('log')\n",
    "ax[11].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[11].set_xticks(x_labels)\n",
    "ax[11].autoscale(True)\n",
    "ax[11].set_ylim([0,600])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharNetMaxout(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (conv1_0): Conv2d(1, 48, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (conv2_0): Conv2d(48, 64, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (conv1_1): Conv2d(1, 48, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (conv2_1): Conv2d(48, 64, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (conv3_0): Conv2d(64, 128, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (conv4_0): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3_1): Conv2d(64, 128, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (conv4_1): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3_2): Conv2d(64, 128, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (conv4_2): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3_3): Conv2d(64, 128, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (conv4_3): Conv2d(128, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=36, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(48, 64, kernel_size=(9, 9), stride=(1, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv2_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
