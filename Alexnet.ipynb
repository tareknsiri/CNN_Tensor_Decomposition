{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the CP-Decomposition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "import numpy as np\n",
    "\n",
    "approx = 0  # Approximation error (mean approximation if multiple decompositions)\n",
    "approx_list = []  # List of approximations in case of several decompositions\n",
    "dim_drop = 0  # Parameters reduction\n",
    "original_param_list = []  # List of original parameters (in case of multiple layers)\n",
    "new_param_list = []  # List of new parameters (in case of multiple layers)\n",
    "\n",
    "def cp_decomposition_conv_layer(layer, rank):\n",
    "\n",
    "    t = tl.tensor(layer.weight.data)\n",
    "    a, b, c, d = np.shape(t)\n",
    "    dim_t = a * b * c * d\n",
    "\n",
    "    # Perform CP decomposition on the layer weight tensorly.\n",
    "    dec = parafac(t, rank=rank, init='svd')\n",
    "\n",
    "    recomp_tensor = tl.kruskal_to_tensor(dec)\n",
    "\n",
    "    norm_t = np.linalg.norm(t)\n",
    "    approx_list.append(np.linalg.norm(recomp_tensor - t) / norm_t)\n",
    "    approx = np.mean(approx_list)\n",
    "\n",
    "    original_param_list.append(dim_t)\n",
    "    new_param_list.append(rank * (a + b + c + d))\n",
    "    dim_drop = sum(original_param_list) / sum(new_param_list)\n",
    "\n",
    "    for i in range(len(dec.factors)):\n",
    "        dec.factors[i] = torch.tensor(dec.factors[i])\n",
    "\n",
    "    last, first, vertical, horizontal = dec.factors\n",
    "\n",
    "    pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels=first.shape[0],\n",
    "                                             out_channels=first.shape[1], kernel_size=1, stride=1, padding=0,\n",
    "                                             dilation=layer.dilation, bias=False)\n",
    "\n",
    "    depthwise_vertical_layer = torch.nn.Conv2d(in_channels=vertical.shape[1],\n",
    "                                               out_channels=vertical.shape[1], kernel_size=(vertical.shape[0], 1),\n",
    "                                               stride=1, padding=(layer.padding[0], 0), dilation=layer.dilation,\n",
    "                                               groups=vertical.shape[1], bias=False)\n",
    "\n",
    "    depthwise_horizontal_layer = \\\n",
    "        torch.nn.Conv2d(in_channels=horizontal.shape[1],\n",
    "                        out_channels=horizontal.shape[1],\n",
    "                        kernel_size=(1, horizontal.shape[0]), stride=layer.stride,\n",
    "                        padding=(0, layer.padding[0]),\n",
    "                        dilation=layer.dilation, groups=horizontal.shape[1], bias=False)\n",
    "\n",
    "    pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels=last.shape[1],\n",
    "                                             out_channels=last.shape[0], kernel_size=1, stride=1,\n",
    "                                             padding=0, dilation=layer.dilation, bias=True)\n",
    "\n",
    "    pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
    "\n",
    "    depthwise_horizontal_layer.weight.data = \\\n",
    "        torch.transpose(horizontal, 1, 0).unsqueeze(1).unsqueeze(1)\n",
    "    depthwise_vertical_layer.weight.data = \\\n",
    "        torch.transpose(vertical, 1, 0).unsqueeze(1).unsqueeze(-1)\n",
    "    pointwise_s_to_r_layer.weight.data = \\\n",
    "        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
    "    pointwise_r_to_t_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer,\n",
    "                  depthwise_horizontal_layer, pointwise_r_to_t_layer]\n",
    "\n",
    "    return nn.Sequential(*new_layers), approx, dim_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decompose(model, rank, layer = 3):\n",
    "    model = model.cpu()\n",
    "\n",
    "    # getting 2-nd convolutional layer\n",
    "    layer2 = model.features[layer]\n",
    "\n",
    "    #decomposing layers\n",
    "    decomp_layers, aprx, ddrop = cp_decomposition_conv_layer(layer2, rank)\n",
    "\n",
    "    #building new Sequential layer in the right order\n",
    "    decomp_features = nn.Sequential(\n",
    "        model.features[0],\n",
    "        model.features[1],\n",
    "        model.features[2],\n",
    "        decomp_layers[0],\n",
    "        decomp_layers[1],\n",
    "        decomp_layers[2],\n",
    "        decomp_layers[3],\n",
    "        model.features[4],\n",
    "        model.features[5],\n",
    "        model.features[6],\n",
    "        model.features[7],\n",
    "        model.features[8],\n",
    "        model.features[9],\n",
    "        model.features[10],\n",
    "        model.features[11],\n",
    "        model.features[12]\n",
    "    )\n",
    "\n",
    "    #changing the model features Sequential with the decomposed one\n",
    "    model.features = decomp_features\n",
    "    \n",
    "    return model, aprx, ddrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting cifar10 dataset and normalizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets/cifar10/'\n",
    "batch_size = 8\n",
    "workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR10(root=data_dir+'train',\n",
    "                          train=True,\n",
    "                          download=True,\n",
    "                          transform=train_transform)\n",
    "testset = datasets.CIFAR10(root=data_dir+'test',\n",
    "                          train=False,\n",
    "                          download=True,\n",
    "                          transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         #num_workers=workers\n",
    "                                         )\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         #num_workers=workers\n",
    "                                        )\n",
    "loaders = {\n",
    "    'train':trainloader,\n",
    "    'test' :testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 6250, 'test': 1250}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {'train': len(trainloader) , 'test': len(testloader)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Fine-tuning & Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _finetune(model, criterion, optimizer, device,  num_epochs=25):\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            total_ = 0\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(loaders[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if idx % 1900 == 0:\n",
    "                            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                                epoch, idx * len(inputs), len(loaders['train'].dataset),\n",
    "                               100. * idx / len(loaders['train']), loss.item()))\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                total_ += labels.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / total_\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}%'.format(\n",
    "                phase, epoch_loss, epoch_acc*100))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}%'.format(best_acc*100))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _test(model, device, x_test = loaders['test']):\n",
    "        model.to(device)\n",
    "        start = time.time()\n",
    "        model.eval()  # network in evaluation mode (for batchnorm and dropout layers)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():  # deactivate the autograd engine to reduce memory usage and speed up\n",
    "            for (data, target) in x_test:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model.forward(data)  # prediction with the CharNet\n",
    "                test_loss += F.cross_entropy(output, target).item()  # Add the negative log likelihood loss.\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(x_test.dataset)\n",
    "        score = int(correct) / len(x_test.dataset)\n",
    "        \n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            test_loss, correct, len(x_test.dataset),\n",
    "            100. * score))\n",
    "        \n",
    "        #return test_loss, score, time.time() - start\n",
    "        return score, time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ALEXNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "want_to_train = False\n",
    "want_to_freeze_layers = False\n",
    "\n",
    "if want_to_train:\n",
    "    #loading pretrained model\n",
    "    model = models.alexnet(pretrained=True)\n",
    "    \n",
    "    #freezing layers\n",
    "    if want_to_freeze_layers:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    #changing its classifier output\n",
    "    model.classifier[6] = nn.Linear(4096, 10)\n",
    "    \n",
    "    #defining training parameters\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 0.00001\n",
    "    num_epochs = 1\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #training\n",
    "    model = _finetune(model, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "    \n",
    "    #saving the model\n",
    "    model.to('cpu')\n",
    "    torch.save(model.state_dict(), './models/alexnet/alexnet_' + datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") +'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP-Decomposing and fine-tunning second convolution of ALEXNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0393, Accuracy: 8888/10000 (89%)\n",
      "\n",
      "Test set: Avg. loss: 0.0393, Accuracy: 8888/10000 (89%)\n",
      "\n",
      "Test set: Avg. loss: 0.0393, Accuracy: 8888/10000 (89%)\n",
      "\n",
      "Test set: Avg. loss: 0.0393, Accuracy: 8888/10000 (89%)\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=4, bias=False)\n",
      "    (5): Conv2d(4, 4, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), groups=4, bias=False)\n",
      "    (6): Conv2d(4, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Test set: Avg. loss: 0.3504, Accuracy: 1021/10000 (10%)\n",
      "\n",
      "Test set: Avg. loss: 0.3504, Accuracy: 1021/10000 (10%)\n",
      "\n",
      "Test set: Avg. loss: 0.3504, Accuracy: 1021/10000 (10%)\n",
      "\n",
      "Test set: Avg. loss: 0.3504, Accuracy: 1021/10000 (10%)\n",
      "Rank: 4, Approx error : 94.41 %,  Acc drop : 88.51 %, Speed-up : 15.56 %, Param red : 288.722\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 10.91 GiB total capacity; 244.88 MiB already allocated; 59.88 MiB free; 250.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c81aa3fa0657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fine-tunning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mft_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_computation_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-6641d1f2c1b4>\u001b[0m in \u001b[0;36m_finetune\u001b[0;34m(model, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_finetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msince\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_model_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 10.91 GiB total capacity; 244.88 MiB already allocated; 59.88 MiB free; 250.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "adress = './models/best/alexnet/alexnet_89.pth'\n",
    "\n",
    "# Loading pretrained & finetuned ALEXNET and setting the output layer \n",
    "model = models.alexnet()\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "model.load_state_dict(torch.load(adress))\n",
    "\n",
    "# parameters\n",
    "ranks_to_decomp = [4,8,16,32,64,128,256,512]\n",
    "layer_to_decomp = 3 #the 2nd conv layer of alexnet is 3\n",
    "n_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# recording computing time of the original model\n",
    "tot_time=0\n",
    "for iter in range(4):\n",
    "    old_acc, old_computation_time = _test(model, \"cpu\")\n",
    "    tot_time += old_computation_time\n",
    "    \n",
    "old_computation_time = tot_time / 4\n",
    "\n",
    "approximation_list, dim_drop_list, acc_list, computation_list, acc_ft_list = [], [], [], [], []\n",
    "\n",
    "\n",
    "for rank in ranks_to_decomp:\n",
    "    # Loading pretrained & finetuned ALEXNET and setting the output layer \n",
    "    model = models.alexnet()\n",
    "    model.classifier[6] = nn.Linear(4096, 10)\n",
    "    model.load_state_dict(torch.load(adress))\n",
    "\n",
    "    model, approximation, dim_drop = _decompose(model, rank, layer_to_decomp)\n",
    "    \n",
    "    # recording computing time of the decomposed model\n",
    "    tot_time = 0\n",
    "    for iter in range(4):\n",
    "        new_acc, new_computation_time = _test(model, \"cpu\")\n",
    "        tot_time += new_computation_time\n",
    "    new_computation_time = tot_time / 4\n",
    "    \n",
    "    #calculating acc drop and speed up\n",
    "    accuracy_drop = (old_acc - new_acc) / old_acc\n",
    "    speed_up = (old_computation_time - new_computation_time) / old_computation_time\n",
    "\n",
    "    approximation_list.append(100 * approximation)\n",
    "    dim_drop_list.append(dim_drop)\n",
    "    acc_list.append(100 * accuracy_drop)\n",
    "    computation_list.append(100 * speed_up)\n",
    "\n",
    "    print(\"Rank: {}, Approx error : {:.2f} %,  Acc drop : {:.2f} %, \"\n",
    "                  \"Speed-up : {:.2f} %, Param red : {:.3f}\\n\".format(rank, 100 * approximation,\n",
    "                                                                          100 * accuracy_drop, 100 * speed_up,\n",
    "                                                                          dim_drop))\n",
    "\n",
    "    # fine-tunning model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    model = _finetune(model, criterion, optimizer, device, num_epochs=n_epochs)\n",
    "\n",
    "    ft_acc, ft_computation_time = _test(model, device)\n",
    "    accuracy_drop = (old_acc - ft_acc) / old_acc\n",
    "    acc_ft_list.append(100 * accuracy_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0393, Accuracy: 8888/10000 (89%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8888, 14.521871328353882)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ranks_to_decomp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-74fbbad04284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranks_to_decomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_scientific\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ranks_to_decomp' is not defined"
     ]
    }
   ],
   "source": [
    "x = ranks_to_decomp\n",
    "\n",
    "formatter = ticker.ScalarFormatter()\n",
    "formatter.set_scientific(False)\n",
    "\n",
    "x_labels = [4, 16, 64,  256]\n",
    "err_ticks = [100, 80, 60 ,40, 20]\n",
    "err_ticks1 = [100, 80, 60, 40, 30, 20 ,0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(16, 4))\n",
    "ax = ax.flatten()\n",
    "\n",
    "plt.suptitle('AlexNet', fontsize=18)\n",
    "\n",
    "ax[0].plot(x, approximation_list, marker='o', mfc='none',  c='#3838D0')\n",
    "ax[0].set_xlabel('Approximation Error (%)')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[0].set_xticks(x_labels)\n",
    "ax[0].set_yticks(err_ticks)\n",
    "\n",
    "\n",
    "ax[1].plot(x, acc_list, marker='o', mfc='none', c='red')\n",
    "ax[1].plot(x, acc_ft_list, 'r--', marker='o', mfc='none')\n",
    "ax[1].set_xlabel('Accuracy Drop (%)')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[1].set_xticks(x_labels)\n",
    "ax[1].set_yscale('linear')\n",
    "ax[1].set_yticks([100, 50, 10, 1])\n",
    "ax[1].yaxis.set_major_formatter(formatter)\n",
    "ax[1].set_ylim([0,100])\n",
    "\n",
    "ax[2].plot(x, computation_list, marker='o', mfc='none', c='green')\n",
    "ax[2].set_xlabel('Speed-up (%)')\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[2].set_xticks(x_labels)\n",
    "ax[2].set_yscale('linear')\n",
    "ax[2].yaxis.set_major_formatter(formatter)\n",
    "ax[2].set_ylim([0,40])\n",
    "\n",
    "\n",
    "ax[3].plot(x, dim_drop_list, marker='o', mfc='none', c='#2C89D0')\n",
    "ax[3].set_xlabel('Parameters reduction (x)')\n",
    "ax[3].set_xscale('log')\n",
    "ax[3].xaxis.set_major_formatter(ticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "ax[3].set_xticks(x_labels)\n",
    "ax[3].set_ylim([0,300])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adress = './models/best/alexnet/alexnet_89.pth'\n",
    "\n",
    "# Loading pretrained & finetuned ALEXNET and setting the output layer \n",
    "model = models.alexnet()\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "model.load_state_dict(torch.load(adress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
